{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F6JAe1nZD0R",
        "outputId": "a6583a8a-470a-4e4c-a288-0b85f00199b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of cleaned text: 100000 characters\n",
            "\n",
            "Sample Text:\n",
            "\n",
            "dventures in Wonderland\n",
            "\n",
            "by Lewis Carroll\n",
            "\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Contents\n",
            "\n",
            " CHAPTER I.     Down the Rabbit-Hole\n",
            " CHAPTER II.    The Pool of Tears\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
            " CHAPTER V.     Advice from a Caterpillar\n",
            " CHAPTER VI.    Pig and Pepper\n",
            " CHAPTER VII.   A Mad Tea-Party\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
            " CHAPTER IX.    The Mock Turtle’s Story\n",
            " CHAPTER X.     The Lobster Quadrille\n",
            " CHAPTER XI.    Wh\n"
          ]
        }
      ],
      "source": [
        "# 1. Dataset Preparation and Preprocessing\n",
        "import requests\n",
        "\n",
        "# Download dataset\n",
        "url = 'https://www.gutenberg.org/files/11/11-0.txt'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    raw_text = response.text\n",
        "else:\n",
        "    raise Exception(\"Failed to download dataset\")\n",
        "\n",
        "# Clean Project Gutenberg header/footer\n",
        "start_phrase = '*** START OF THIS PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***'\n",
        "end_phrase = '*** END OF THIS PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***'\n",
        "\n",
        "start_idx = raw_text.find(start_phrase) + len(start_phrase)\n",
        "end_idx = raw_text.find(end_phrase)\n",
        "\n",
        "# Extract and clean the main content\n",
        "clean_text = raw_text[start_idx:end_idx].strip()\n",
        "\n",
        "# Limit text size for faster processing (optional)\n",
        "text = clean_text[:100000]\n",
        "\n",
        "# Display some info\n",
        "print(f\"Length of cleaned text: {len(text)} characters\")\n",
        "print(\"\\nSample Text:\\n\")\n",
        "print(text[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Exploring Generative Pre-trained Transformers (GPTs)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assume `text` is already loaded and cleaned (from previous steps)\n",
        "\n",
        "# 1. Create character vocabulary and mappings\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {c: i for i, c in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# 2. Create input-target sequences\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    return chunk[:-1], chunk[1:]\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# 3. Shuffle and batch the dataset\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# 5. Define loss function and compile model\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# 6. Train the model\n",
        "EPOCHS = 5\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGq4WRBRZV8T",
        "outputId": "0fc51939-a549-46a6-829f-010dc2d62774"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - loss: 3.8637\n",
            "Epoch 2/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - loss: 3.1290\n",
            "Epoch 3/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 2.9129\n",
            "Epoch 4/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - loss: 2.6885\n",
            "Epoch 5/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 2.5368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Application Demonstration: Content Creation Tool\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Rebuild the model for text generation (batch size = 1)\n",
        "model_gen = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Build with input shape for single prediction\n",
        "model_gen.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Set weights from the trained model\n",
        "model_gen.set_weights(model.get_weights())\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, start_string, num_generate=400, temperature=1.0):\n",
        "    input_eval = [char2idx.get(s, 0) for s in start_string]  # Map to int IDs\n",
        "    input_eval = tf.expand_dims(input_eval, 0)  # Batch dimension\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :]  # Last character prediction\n",
        "        predictions = predictions / temperature  # Adjust temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Add predicted character\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "        # Feed prediction back as next input\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Generate example text\n",
        "prompt = \"Once upon a time, there was a curious rabbit\"\n",
        "generated_text = generate_text(model_gen, prompt, temperature=0.4)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDOoNAW1ZV4V",
        "outputId": "f9e5a589-c02b-4d09-cf13-49e1e37b727c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, there was a curious rabbit o athe an whe helin on’dN\n",
            "_, ig?F“n he the hendMbZKbff-rs beron;’dushe haghe ad t us aro the and wan oun he he andthe wad\n",
            "s;;’tit onrm_on he ond llongus he t Int st r mywheg an ithithed h?GKke hingm_ s id athe ad he wn he weshan shee the an fe he h?XThs\n",
            "“M)gulD“Wre t whe l s.Rje the athe athe and t an the t ingas he wlen thetound s thegle th, the Is he thin adombn the aty””?VI at he he b)in win!‘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYffIRhcZV2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6B9_7O9xZV0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XppS3iipZVxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}