{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FyMLmBqTm3UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/Daily Historical Stock Prices (1970 - 2018).zip'\n",
        "\n",
        "# Check if it's a valid zip file\n",
        "if zipfile.is_zipfile(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "        print(\"Zip file extracted.\")\n",
        "else:\n",
        "    print(\"Error: The file is not a valid zip file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmce2nM0Ye6I",
        "outputId": "d268d7c5-8b0f-40d2-f6f4-321eb0bceea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file is not a valid zip file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload and initialize the datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the datasets\n",
        "historical_stocks = pd.read_csv('historical_stocks.csv')\n",
        "historical_stock_prices = pd.read_csv('historical_stock_prices.csv')\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(\"Historical Stocks shape:\", historical_stocks.shape)\n",
        "print(\"Historical Stock Prices shape:\", historical_stock_prices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwCbVQnTm3Q8",
        "outputId": "549c49e4-7d8e-4343-c2fe-2f4c8c5bac44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Historical Stocks shape: (6460, 5)\n",
            "Historical Stock Prices shape: (20973889, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Advanced Data Cleaning - Handle Missing Values\n",
        "print(\"=== ADVANCED DATA CLEANING ===\")\n",
        "print(\"\\\n",
        "1. Missing Values Analysis:\")\n",
        "\n",
        "# Check missing values in both datasets\n",
        "print(\"Historical Stocks missing values:\")\n",
        "stocks_missing = historical_stocks.isnull().sum()\n",
        "print(stocks_missing)\n",
        "\n",
        "print(\"\\\n",
        "Historical Stock Prices missing values:\")\n",
        "prices_missing = historical_stock_prices.isnull().sum()\n",
        "print(prices_missing)\n",
        "\n",
        "# Handle missing values in historical_stocks\n",
        "print(\"\\\n",
        "2. Handling Missing Values:\")\n",
        "print(\"Before cleaning - Stocks dataset shape:\", historical_stocks.shape)\n",
        "\n",
        "# Fill missing sector and industry with 'Unknown'\n",
        "historical_stocks['sector'] = historical_stocks['sector'].fillna('Unknown')\n",
        "historical_stocks['industry'] = historical_stocks['industry'].fillna('Unknown')\n",
        "\n",
        "print(\"After filling missing values - Stocks dataset:\")\n",
        "print(historical_stocks.isnull().sum())\n",
        "\n",
        "# Convert date column to datetime\n",
        "historical_stock_prices['date'] = pd.to_datetime(historical_stock_prices['date'])\n",
        "print(\"\\\n",
        "Date conversion completed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okwA-lNUm3Nk",
        "outputId": "ba798e46-6dd9-4cd5-831c-145cc7dc482d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ADVANCED DATA CLEANING ===\n",
            "1. Missing Values Analysis:\n",
            "Historical Stocks missing values:\n",
            "ticker         0\n",
            "exchange       0\n",
            "name           0\n",
            "sector      1440\n",
            "industry    1440\n",
            "dtype: int64\n",
            "Historical Stock Prices missing values:\n",
            "ticker       0\n",
            "open         0\n",
            "close        0\n",
            "adj_close    0\n",
            "low          0\n",
            "high         0\n",
            "volume       0\n",
            "date         0\n",
            "dtype: int64\n",
            "2. Handling Missing Values:\n",
            "Before cleaning - Stocks dataset shape: (6460, 5)\n",
            "After filling missing values - Stocks dataset:\n",
            "ticker      0\n",
            "exchange    0\n",
            "name        0\n",
            "sector      0\n",
            "industry    0\n",
            "dtype: int64\n",
            "Date conversion completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Outlier Detection and Analysis\n",
        "print(\"3. Outlier Detection and Analysis:\")\n",
        "\n",
        "# Focus on key numerical columns for outlier detection\n",
        "numerical_cols = ['open', 'close', 'adj_close', 'low', 'high', 'volume']\n",
        "\n",
        "# Calculate basic statistics\n",
        "print(\"Basic statistics for numerical columns:\")\n",
        "stats_summary = historical_stock_prices[numerical_cols].describe()\n",
        "print(stats_summary)\n",
        "\n",
        "# Detect outliers using IQR method for close prices and volume\n",
        "def detect_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "# Detect outliers in close prices\n",
        "close_outliers, close_lower, close_upper = detect_outliers_iqr(historical_stock_prices, 'close')\n",
        "print(f\"\\\n",
        "Close price outliers: {len(close_outliers)} records\")\n",
        "print(f\"Close price bounds: [{close_lower:.2f}, {close_upper:.2f}]\")\n",
        "\n",
        "# Detect outliers in volume\n",
        "volume_outliers, vol_lower, vol_upper = detect_outliers_iqr(historical_stock_prices, 'volume')\n",
        "print(f\"Volume outliers: {len(volume_outliers)} records\")\n",
        "print(f\"Volume bounds: [{vol_lower:.0f}, {vol_upper:.0f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5iDaieD4TMS",
        "outputId": "fbd71029-21a6-4485-e544-8fc4d593bf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Outlier Detection and Analysis:\n",
            "Basic statistics for numerical columns:\n",
            "               open         close     adj_close           low          high  \\\n",
            "count  2.097389e+07  2.097389e+07  2.097389e+07  2.097389e+07  2.097389e+07   \n",
            "mean   7.605823e+01  7.611403e+01  1.481184e+14  7.422064e+01  7.803857e+01   \n",
            "std    2.849639e+03  2.870159e+03  4.574674e+16  2.746059e+03  2.997937e+03   \n",
            "min    4.000000e-04  2.000000e-04  2.282650e-09  1.000000e-04  4.000000e-04   \n",
            "25%    7.500000e+00  7.500000e+00  4.620000e+00  7.360000e+00  7.630000e+00   \n",
            "50%    1.545000e+01  1.545000e+01  1.138199e+01  1.524000e+01  1.566000e+01   \n",
            "75%    2.972000e+01  2.972000e+01  2.472046e+01  2.928000e+01  3.010000e+01   \n",
            "max    2.034000e+06  1.779750e+06  1.894962e+19  1.440000e+06  2.070000e+06   \n",
            "\n",
            "             volume  \n",
            "count  2.097389e+07  \n",
            "mean   1.227043e+06  \n",
            "std    1.316686e+07  \n",
            "min    1.000000e+00  \n",
            "25%    2.210000e+04  \n",
            "50%    1.260000e+05  \n",
            "75%    6.074000e+05  \n",
            "max    4.483504e+09  \n",
            "Close price outliers: 1694375 records\n",
            "Close price bounds: [-25.83, 63.05]\n",
            "Volume outliers: 2911215 records\n",
            "Volume bounds: [-855850, 1485350]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate the extreme values more carefully\n",
        "print(\"4. Detailed Outlier Investigation:\")\n",
        "\n",
        "# Check for impossible values (negative prices, zero volume patterns)\n",
        "print(\"Data quality checks:\")\n",
        "print(f\"Negative open prices: {(historical_stock_prices['open'] < 0).sum()}\")\n",
        "print(f\"Negative close prices: {(historical_stock_prices['close'] < 0).sum()}\")\n",
        "print(f\"Negative high prices: {(historical_stock_prices['high'] < 0).sum()}\")\n",
        "print(f\"Negative low prices: {(historical_stock_prices['low'] < 0).sum()}\")\n",
        "print(f\"Zero volume records: {(historical_stock_prices['volume'] == 0).sum()}\")\n",
        "\n",
        "# Check for logical inconsistencies (high < low, etc.)\n",
        "logical_errors = historical_stock_prices[\n",
        "    (historical_stock_prices['high'] < historical_stock_prices['low']) |\n",
        "    (historical_stock_prices['close'] > historical_stock_prices['high']) |\n",
        "    (historical_stock_prices['close'] < historical_stock_prices['low']) |\n",
        "    (historical_stock_prices['open'] > historical_stock_prices['high']) |\n",
        "    (historical_stock_prices['open'] < historical_stock_prices['low'])\n",
        "]\n",
        "print(f\"Logical inconsistencies: {len(logical_errors)} records\")\n",
        "\n",
        "# Check the adj_close column which seems to have extreme values\n",
        "print(f\"\\\n",
        "Adjusted close statistics:\")\n",
        "print(f\"Min: {historical_stock_prices['adj_close'].min()}\")\n",
        "print(f\"Max: {historical_stock_prices['adj_close'].max()}\")\n",
        "print(f\"Mean: {historical_stock_prices['adj_close'].mean()}\")\n",
        "\n",
        "# Sample some extreme adj_close values\n",
        "extreme_adj = historical_stock_prices.nlargest(10, 'adj_close')[['ticker', 'date', 'close', 'adj_close']]\n",
        "print(\"\\\n",
        "Top 10 extreme adjusted close values:\")\n",
        "print(extreme_adj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exZUbCxq4TIP",
        "outputId": "991616a9-9b2e-4c30-922c-1335e48a5eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Detailed Outlier Investigation:\n",
            "Data quality checks:\n",
            "Negative open prices: 0\n",
            "Negative close prices: 0\n",
            "Negative high prices: 0\n",
            "Negative low prices: 0\n",
            "Zero volume records: 0\n",
            "Logical inconsistencies: 2076 records\n",
            "Adjusted close statistics:\n",
            "Min: 2.2826502910078303e-09\n",
            "Max: 1.89496189108124e+19\n",
            "Mean: 148118439358479.7\n",
            "Top 10 extreme adjusted close values:\n",
            "        ticker       date     close     adj_close\n",
            "6174033    AAN 1987-02-27  1.296296  1.894962e+19\n",
            "6174039    AAN 1987-03-10  1.296296  1.894962e+19\n",
            "6174040    AAN 1987-03-11  1.296296  1.894962e+19\n",
            "6174041    AAN 1987-03-12  1.296296  1.894962e+19\n",
            "6174044    AAN 1987-03-17  1.296296  1.894962e+19\n",
            "6174045    AAN 1987-03-18  1.296296  1.894962e+19\n",
            "6174046    AAN 1987-03-19  1.296296  1.894962e+19\n",
            "6174049    AAN 1987-03-25  1.296296  1.894962e+19\n",
            "6174051    AAN 1987-03-27  1.287037  1.881428e+19\n",
            "6174050    AAN 1987-03-26  1.268519  1.854356e+19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data Cleaning and Error Correction\n",
        "print(\"5. Data Cleaning and Error Correction:\")\n",
        "\n",
        "# Create a copy for cleaning\n",
        "cleaned_prices = historical_stock_prices.copy()\n",
        "\n",
        "# Remove logical inconsistencies\n",
        "print(f\"Before removing logical errors: {len(cleaned_prices)} records\")\n",
        "cleaned_prices = cleaned_prices[\n",
        "    (cleaned_prices['high'] >= cleaned_prices['low']) &\n",
        "    (cleaned_prices['close'] <= cleaned_prices['high']) &\n",
        "    (cleaned_prices['close'] >= cleaned_prices['low']) &\n",
        "    (cleaned_prices['open'] <= cleaned_prices['high']) &\n",
        "    (cleaned_prices['open'] >= cleaned_prices['low'])\n",
        "]\n",
        "print(f\"After removing logical errors: {len(cleaned_prices)} records\")\n",
        "\n",
        "# Handle extreme adjusted close values - cap at reasonable bounds\n",
        "# Calculate reasonable bounds based on close prices\n",
        "adj_close_q99 = cleaned_prices['close'].quantile(0.99)\n",
        "adj_close_q01 = cleaned_prices['close'].quantile(0.01)\n",
        "\n",
        "print(f\"Close price 99th percentile: {adj_close_q99:.2f}\")\n",
        "print(f\"Close price 1st percentile: {adj_close_q01:.2f}\")\n",
        "\n",
        "# Cap adjusted close values to reasonable bounds\n",
        "cleaned_prices['adj_close'] = np.where(\n",
        "    cleaned_prices['adj_close'] > adj_close_q99 * 10,  # Allow 10x the 99th percentile\n",
        "    cleaned_prices['close'],  # Replace with close price\n",
        "    cleaned_prices['adj_close']\n",
        ")\n",
        "\n",
        "cleaned_prices['adj_close'] = np.where(\n",
        "    cleaned_prices['adj_close'] < adj_close_q01 / 10,  # Allow 1/10th the 1st percentile\n",
        "    cleaned_prices['close'],  # Replace with close price\n",
        "    cleaned_prices['adj_close']\n",
        ")\n",
        "\n",
        "print(f\"After capping extreme adj_close values:\")\n",
        "print(f\"New adj_close max: {cleaned_prices['adj_close'].max():.2f}\")\n",
        "print(f\"New adj_close min: {cleaned_prices['adj_close'].min():.2f}\")\n",
        "\n",
        "# Remove duplicate records\n",
        "print(f\"\\\n",
        "Before removing duplicates: {len(cleaned_prices)} records\")\n",
        "cleaned_prices = cleaned_prices.drop_duplicates(subset=['ticker', 'date'])\n",
        "print(f\"After removing duplicates: {len(cleaned_prices)} records\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T5T_OCM4TFI",
        "outputId": "a22e7b6e-c562-49bc-90c0-480ec95453f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Data Cleaning and Error Correction:\n",
            "Before removing logical errors: 20973889 records\n",
            "After removing logical errors: 20971813 records\n",
            "Close price 99th percentile: 282.00\n",
            "Close price 1st percentile: 0.59\n",
            "After capping extreme adj_close values:\n",
            "New adj_close max: 1779750.00\n",
            "New adj_close min: 0.00\n",
            "Before removing duplicates: 20971813 records\n",
            "After removing duplicates: 20971813 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Feature Engineering\n",
        "print(\"=== DATA TRANSFORMATION ===\")\n",
        "print(\"\\\n",
        "6. Feature Engineering:\")\n",
        "\n",
        "# Sort data by ticker and date for time series calculations\n",
        "cleaned_prices = cleaned_prices.sort_values(['ticker', 'date'])\n",
        "\n",
        "# Calculate rolling averages (5, 10, 20, 50 day moving averages)\n",
        "print(\"Creating rolling averages...\")\n",
        "for window in [5, 10, 20, 50]:\n",
        "    cleaned_prices[f'ma_{window}'] = cleaned_prices.groupby('ticker')['close'].transform(\n",
        "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "# Calculate volatility measures\n",
        "print(\"Creating volatility measures...\")\n",
        "cleaned_prices['daily_return'] = cleaned_prices.groupby('ticker')['close'].pct_change()\n",
        "cleaned_prices['volatility_10d'] = cleaned_prices.groupby('ticker')['daily_return'].transform(\n",
        "    lambda x: x.rolling(window=10, min_periods=1).std()\n",
        ")\n",
        "cleaned_prices['volatility_30d'] = cleaned_prices.groupby('ticker')['daily_return'].transform(\n",
        "    lambda x: x.rolling(window=30, min_periods=1).std()\n",
        ")\n",
        "\n",
        "# Calculate technical indicators\n",
        "print(\"Creating technical indicators...\")\n",
        "\n",
        "# RSI (Relative Strength Index) - simplified version\n",
        "def calculate_rsi(prices, window=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "cleaned_prices['rsi'] = cleaned_prices.groupby('ticker')['close'].transform(calculate_rsi)\n",
        "\n",
        "# Price momentum (price change over different periods)\n",
        "for period in [1, 5, 10, 20]:\n",
        "    cleaned_prices[f'momentum_{period}d'] = cleaned_prices.groupby('ticker')['close'].transform(\n",
        "        lambda x: x.pct_change(periods=period)\n",
        "    )\n",
        "\n",
        "# Volume indicators\n",
        "cleaned_prices['volume_ma_10'] = cleaned_prices.groupby('ticker')['volume'].transform(\n",
        "    lambda x: x.rolling(window=10, min_periods=1).mean()\n",
        ")\n",
        "cleaned_prices['volume_ratio'] = cleaned_prices['volume'] / cleaned_prices['volume_ma_10']\n",
        "\n",
        "print(f\"Feature engineering completed. New shape: {cleaned_prices.shape}\")\n",
        "print(f\"New features created: {cleaned_prices.shape[1] - 8} additional columns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wZKuEVo4TCA",
        "outputId": "69a51f3b-8c44-422b-d290-9f7ba3a787d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATA TRANSFORMATION ===\n",
            "6. Feature Engineering:\n",
            "Creating rolling averages...\n",
            "Creating volatility measures...\n",
            "Creating technical indicators...\n",
            "Feature engineering completed. New shape: (20971813, 22)\n",
            "New features created: 14 additional columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxTZjQqj4yYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDGAYQOD4yU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjfsmaSw4yRd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}